<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-05-10T00:17:24-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Hemalatha Bhaskar</title><subtitle>Data Analyst Portfolio</subtitle><entry><title type="html">Immigration Dashboard - Tableau</title><link href="http://localhost:4000/project/Sankey-dashboard" rel="alternate" type="text/html" title="Immigration Dashboard - Tableau" /><published>2024-03-02T00:00:00-05:00</published><updated>2024-03-02T00:00:00-05:00</updated><id>http://localhost:4000/project/Sankey-dashboard</id><content type="html" xml:base="http://localhost:4000/project/Sankey-dashboard"><![CDATA[<hr />

<p>The project began with the creation of a union with an exact copy of the original data. Calculated fields were then developed to facilitate the construction of the Sankey chart.</p>

<h6 target="blank" id="source-code--sankey_dashboardtwbx">Source Code : <a href="https://github.com/hemabhaskar/Project/blob/main/Sankey_Dashboard.twbx"><code class="language-plaintext highlighter-rouge">Sankey_Dashboard.twbx</code></a></h6>
<h6 id="tableau-live--sankey_dashboard_live">Tableau Live : <a href="https://public.tableau.com/app/profile/hemalatha.bhaskar/viz/SankeyDashboard_17117628102310/SankeyDashboard"><code class="language-plaintext highlighter-rouge">Sankey_Dashboard_Live</code></a></h6>
<p><br /></p>

<h4 id="developed-calculated-fields">Developed calculated fields</h4>
<p>“ToPad” to create values 1 and 49 for filling gaps.
Implemented binning to bridge the gap between the created values.
Introduced a new variable “T” to facilitate the spread of Sankey marks.
Computed running sums of immigrants divided by the total number for the start and end of the Sankey chart.
Derived a Sigmoid curve to define the curve of the Sankey chart noodles.</p>

<p>Building the visualization involved utilizing two dimensions, Countries and Year, to create the Sankey chart. Finally, all sheets were integrated to display the flow of immigrants from countries to years, also providing insights into the percentage of total immigrants from 2004 to 2013.</p>

<h4 id="overall-findings">Overall Findings</h4>
<p>The visualization highlights a notable trend: the percentage of total immigrants to Canada has been consistently increasing across the years. Notably, China emerges as the leading contributor, consistently supplying the largest number of immigrants to Canada throughout the analyzed period.</p>]]></content><author><name></name></author><category term="sample" /><category term="post" /><category term="test" /><summary type="html"><![CDATA[This Tableau project shows migration to Canada with a Sankey dashboard using immigration data from Kaggle from 2004 to 2013. The top five nations that send immigrants are the UK, China, India, Pakistan, and the Philippines. The Sankey chart shows the increasing percentage of total immigrants throughout time, together with the flow of immigrants, using dimensions of Countries and Year. Notably, China has been Canada's top source of immigrants on a constant basis.]]></summary></entry><entry><title type="html">HR Dashboard - PowerBI</title><link href="http://localhost:4000/project/hr-dashboard-copy" rel="alternate" type="text/html" title="HR Dashboard - PowerBI" /><published>2024-03-01T00:00:00-05:00</published><updated>2024-03-01T00:00:00-05:00</updated><id>http://localhost:4000/project/hr-dashboard%20copy</id><content type="html" xml:base="http://localhost:4000/project/hr-dashboard-copy"><![CDATA[<hr />

<p>This HR Analytics Dashboard project involved meticulous data preparation, advanced modeling using Power BI, and insightful data analysis through DAX functions. The resulting visualizations brought to light key trends, such as a demographic concentration in the 30s and 40s, attrition concerns in specific departments, and the prominence of the Production department. The dashboard also revealed salary dynamics and job satisfaction insights, emphasizing the contentment of “Product Technician I.” Recommendations include strategies for attrition mitigation, recruitment enhancement, and tailored department-specific interventions. With a clear focus on geographical talent management, the project underscores the importance of optimizing the salary structure and positions itself as a valuable tool for informed HR decision-making and continuous optimization.</p>

<h6 target="blank" id="source-code--hr_analytics-dashboardpbix">Source Code : <a href="https://github.com/hemabhaskar/Project/blob/main/HR_Analytics%20dashboard.pbix"><code class="language-plaintext highlighter-rouge">HR_Analytics dashboard.pbix</code></a></h6>

<h3 id="process-involved">Process involved</h3>

<p>1️. Data Preparation: Ensured the data was clean, organized, and ready for analysis.<br />
2️. Data Modeling: Structured the data for effective analysis using Power BI.<br />
3️. Data Analysis (DAX): Leveraged DAX functions to dig deep into the numbers.<br />
4️. Data Visualization (Dashboard): Created visually compelling dashboards to make complex data easily understandable.<br />
5️. Insights: Unearthed some fascinating findings!</p>

<h3 id="insights">Insights</h3>
<h4 id="employee-demographics-analysis">Employee Demographics Analysis</h4>
<p>Employee distribution indicates a concentration in the 30s and 40s age groups.
Female employees dominate the 30s, while male employees prevail in the 40s.</p>

<h4 id="attrition-and-recruitment-trends">Attrition and Recruitment Trends</h4>
<p>Attrition rate calculated; a potential concern, especially in specific departments.Recruitment channels evaluated, with insights into the most effective sources.</p>

<h4 id="department-specific-observations">Department-specific Observations</h4>
<p>Production department stands out with the highest employee count
Attrition rates vary across departments, with Production showing notable figures.</p>

<h4 id="salary-and-satisfaction-dynamics">Salary and Satisfaction Dynamics</h4>
<p>Average salary visualized, revealing insights into the company’s overall compensation structure.
Job satisfaction ratings explored, with “Product Technician I” emerging as the most satisfied position.</p>

<h4 id="geographical-employee-distribution">Geographical Employee Distribution</h4>
<p>Employee distribution across states visualized, highlighting State MA as having the highest number of employees.</p>

<h3 id="recommendations">Recommendations</h3>
<h4 id="attrition-mitigation">Attrition Mitigation</h4>
<p>Focus on retention strategies, especially in high-attrition departments.
Consider personalized incentives for employees in critical roles.</p>

<h4 id="recruitment-enhancement">Recruitment Enhancement</h4>
<p>Strengthen recruitment from successful channels.
Leverage insights to refine recruitment strategies for specific departments.</p>

<h4 id="department-specific-strategies">Department-specific Strategies</h4>
<p>Address factors contributing to attrition in the Production department.
Investigate job satisfaction drivers for Product Technician I to replicate success in other roles.</p>

<h4 id="geographical-talent-management">Geographical Talent Management</h4>
<p>Explore reasons behind the concentration of employees in State MA.
Consider tailoring HR strategies based on regional employee dynamics.</p>

<h4 id="salary-structure-optimization">Salary Structure Optimization</h4>
<p>Evaluate the salary structure against industry benchmarks.
Introduce targeted adjustments to address potential disparities.</p>

<h3 id="inferences-and-takeaways">Inferences and Takeaways</h3>
<p>The HR Management Dashboard provides a nuanced understanding of employee dynamics, allowing for strategic interventions. Identified trends and patterns pave the way for targeted HR initiatives to enhance employee satisfaction, mitigate attrition, and optimize recruitment strategies.</p>]]></content><author><name></name></author><category term="sample" /><category term="post" /><category term="test" /><summary type="html"><![CDATA[Transformed raw HR data from the Human Resources Dataset on Kaggle into a comprehensive HR Management Dashboard using Power BI. This project involved data cleaning, visualization creation, and deriving meaningful insights to empower HR decision-makers.]]></summary></entry><entry><title type="html">Data analysis - MySQL</title><link href="http://localhost:4000/project/sql_data_analysis" rel="alternate" type="text/html" title="Data analysis - MySQL" /><published>2024-02-01T00:00:00-05:00</published><updated>2024-02-01T00:00:00-05:00</updated><id>http://localhost:4000/project/sql_data_analysis</id><content type="html" xml:base="http://localhost:4000/project/sql_data_analysis"><![CDATA[<p target="blank">The SQL project at VivaK involved importing and harmonizing diverse data formats into the ‘vivakdump’ schema, serving as a staging ground. Subsequently, the meticulously crafted ‘vivakhr’ schema was created, ensuring clean and organized data for analysis. Tasks included handling duplicates, aligning data types, and filling missing values to maintain database integrity. Refinement of columns like ‘report_to’ and strategic handling of missing salary entries were key highlights. Additional tasks involved calculating time differences, generating random performance ratings, and updating salary calculations. Overall, the project showcased adept data management strategies, laying the foundation for comprehensive analysis within VivaK’s OLAP system.</p>
<h6 id="source-code--vivakhr_analysissql">Source Code : <a href="https://github.com/hemabhaskar/Project/blob/main/Vivakhr_analaysis.sql"><code class="language-plaintext highlighter-rouge">Vivakhr_analysis.sql</code></a></h6>

<h2 id="key-steps-and-analysis">Key Steps and Analysis</h2>

<p>Given the complexity of the data management at VivaK, which involved multiple formats and anomalies, a meticulous plan was devised by importing data from various sources into a schema named ‘vivakdump.’ This schema served as a staging ground for harmonizing data from JSON, CSV, and Excel files.</p>

<p>The process unfolded in three steps</p>
<h5 id="1-sql-file-upload">1. SQL File Upload<br /></h5>
<p>Initiated the process by uploading the SQL file (HR) containing essential data (countries, locations, and regions). This formed the foundational layer for subsequent integration.</p>
<h5 id="2-importing-diverse-data-formats">2. Importing Diverse Data Formats<br /></h5>
<p>Systematically, have imported data from JSON (employees), CSV (dependent), and Excel (orgstructure) files into the ‘vivakdump’ schema. This ensured seamless coexistence of data from different sources.</p>
<h5 id="3-creation-of-final-vivakhr-schema">3. Creation of Final ‘vivakhr’ Schema<br /></h5>
<p>With all data consolidated in the ‘vivakdump’ schema, the final ‘vivakhr’ schema has been created. This schema, characterized by clean and organized data, led to the well-designed OLAP database for VivaK.</p>

<h3 id="schema-design-process">Schema Design Process</h3>
<p>Entities:
Identify entities based on the narrative. Examples include Employees, Departments, Locations, etc.</p>

<h3 id="attributes">Attributes</h3>
<p>List the attributes for each entity. For example, Employee attributes might include employee_id, employee_name, hire_date, etc.</p>

<h3 id="constraints">Constraints</h3>
<p>Identify and define primary keys, not null columns, and unique keys for each entity.
Handle the “location_id” column issue in the Employees data.</p>

<h3 id="relationships-and-foreign-keys">Relationships and Foreign Keys</h3>
<p>Establish relationships between entities (e.g., Employees report to Managers).
Define foreign keys based on relationships.</p>

<h3 id="schema-creation">Schema Creation</h3>
<p>The process commenced by checking for the existence of the ‘vivakhr’ schema. If found, it was promptly dropped to ensure a clean slate for subsequent actions. Following this, the necessary tables, including regions, countries, locations, departments, jobs, employees, and dependents, were meticulously created within the newly established ‘vivakhr’ schema. This systematic approach laid the foundation for a well-structured schema, facilitating comprehensive data analysis and reporting in VivaK’s OLAP database.</p>

<h3 id="import-and-clean-data">Import and Clean Data</h3>
<p>Data import into the ‘vivakhr’ schema tables from the ‘vivakdump’ schema was seamlessly executed. The process encompassed addressing duplicates, aligning data with designated data types, and resolving missing values. Key actions involved standardizing phone numbers and dates, and confirming the double data type for all salary-related columns. This meticulous approach ensured the integrity and coherence of the database, laying the groundwork for robust analytics within VivaK’s OLAP system.</p>

<p>Data Insertion into vivakhr Schema Tables:
Data insertion into tables involved careful selection of records from corresponding tables in the ‘vivakdump’ schema.</p>

<h3 id="handling-missing-values">Handling Missing Values</h3>
<p>A thorough examination of the data at hand resulted in the “report_to” column in the “employees” table being refined, guaranteeing that correct managerial information was effortlessly incorporated. Additionally, a creative solution was used to solve the missing entries in the salary column which determined the mean of “min_salary” and “max_salary,” offering a thorough way to guarantee accuracy and completeness of data in VivaK’s OLAP system.</p>

<h3 id="handle-duplicates">Handle Duplicates</h3>
<p>Ensured that there is no redundant data in the tables and that they are fully normalized. Use qualifying candidate keys to detect and deal with duplicates.</p>

<h3 id="calculations-and-updates">Calculations and Updates</h3>

<ul>
  <li>
    <p>experience_at_VivaK
Calculate the time difference (in months) between the hire date and the current date for each employee and update the column.</p>
  </li>
  <li>
    <p>last_performance_rating
Generate random performance ratings for each employee and update the column.</p>
  </li>
  <li>
    <p>salary_after_increment
Calculate and update the salary_after_increment column based on the provided formulas.</p>
  </li>
  <li>
    <p>annual_dependent_benefit
Calculate and update the annual_dependent_benefit column based on the specified rules</p>
  </li>
  <li>
    <p>Email Update
Replace employee email addresses with the new domain ‘vivaK.com’.</p>
  </li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="informative" /><category term="technology" /><summary type="html"><![CDATA[The objective is to demonstrate SQL proficiency by creating an OLAP Database for VivaK, a retail chain in Southlake, Texas, USA. Tasks include analyzing data, designing a MySQL Data Model, schema creation, data import and cleaning, and performing complex calculations. Key objectives involve handling duplicates, formatting data types, treating missing values, and calculating metrics like experience_at_VivaK and last_performance_rating. This project showcases expertise in SQL data modeling, schema creation, data cleaning, and complex calculations for real-world business analytics challenges.]]></summary></entry><entry><title type="html">Multinomial Logical Regression - Python</title><link href="http://localhost:4000/project/multinomial_regression" rel="alternate" type="text/html" title="Multinomial Logical Regression - Python" /><published>2024-01-01T00:00:00-05:00</published><updated>2024-01-01T00:00:00-05:00</updated><id>http://localhost:4000/project/multinomial_regression</id><content type="html" xml:base="http://localhost:4000/project/multinomial_regression"><![CDATA[<p>Furthermore, the analysis is enriched with profound insights into the predictive capabilities of the Multinomial Regression model. This involves examining the precision, recall, F1-score, and other performance metrics derived from the Confusion Matrix. These metrics offer valuable insights into the model’s ability to correctly identify positive and negative sentiment in the book reviews.</p>

<h6 target="blank" id="source-code--sentimental_analysispy">Source Code : <a href="https://github.com/hemabhaskar/Project/blob/main/Sentimental_analysis.py"><code class="language-plaintext highlighter-rouge">Sentimental_analysis.py</code></a></h6>

<h2 id="multinomial-regression">Multinomial Regression</h2>
<h4 id="step-1-identifying-variables">Step 1: Identifying Variables</h4>

<p>The first step involved identifying the variables essential for the multinomial regression analysis. This includes selecting predictor variables (features) that could potentially influence the rating of books, as well as the target variable, which in this case is the book rating.</p>

<h4 id="step-2-data-preparation">Step 2: Data Preparation</h4>

<p>Data preparation was crucial for ensuring the quality and suitability of the dataset for analysis. This included removing stopwords and punctuation from the text data to clean and streamline the text for analysis. Additionally, TF/IDF vectorization was applied to convert the text data into numerical vectors, which could be used as input for the regression model.</p>

<h4 id="step-3-datasets">Step 3: Datasets</h4>

<p>The dataset was divided into two subsets: the training dataset and the test dataset. The training dataset was used to train the multinomial regression model, while the test dataset was reserved for evaluating the model’s performance.</p>

<h4 id="step-4-predictive-analysis">Step 4: Predictive Analysis</h4>

<p>In this step, the multinomial regression model was trained using the training dataset. The model learned the relationships between the predictor variables and the book ratings. Once trained, the model was used to predict the ratings of books in the test dataset.</p>
<li> True Positive (TP): Identified 92633 instances correctly. </li>
<li> False Positive (FP): Totalled 37315, comprising errors in positive prediction.</li>
<li>False Negative (FN): Totalled 7317, indicating missed positive identifications.</li>
<li>These metrics collectively represent the performance of the classification model in predicting positive instances.</li>
<image src="https://raw.githubusercontent.com/hemabhaskar/portfolio/gh-pages/assets/images/conf_matrix_calc.png" width="480" height="259" frameborder="0"></image>
<li>Precision: 71% of positive predictions were correct.</li>
<li>Recall: 93% of actual positives were identified.</li>
<li>F1 Score: Balanced at 0.81, considering precision and recall.</li>
<li>Overall Accuracy: Classified 65% of instances correctly.</li>
<li>Support: Dataset contains 99950 instances, with a total test sample of 169229.</li>

<image src="https://raw.githubusercontent.com/hemabhaskar/portfolio/gh-pages/assets/images/class_report.png" width="480" height="1500" frameborder="0"></image>

<h4 id="step-5-model-evaluation">Step 5: Model Evaluation</h4>

<p>The performance of the multinomial regression model was evaluated using various metrics, including the classification report and confusion matrix. These metrics provided insights into the model’s precision, recall, and overall accuracy in predicting book ratings.</p>

<h4 id="step-6-overall-findings">Step 6: Overall Findings</h4>

<p>Upon evaluating the model, it was observed that the model performed well in predicting books with a 5-star rating, achieving a precision of 71%. However, approximately 29% of books that were not actually rated 5 stars were incorrectly classified as such. The model also demonstrated a high recall rate of 93%, indicating that it effectively captured a large proportion of actual 5-star rated books. Overall, the model showed promise in identifying 5-star rated books but may not be as accurate for predicting other ratings.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="informative" /><category term="technology" /><summary type="html"><![CDATA[This project embarks on a thorough sentiment analysis journey using Python, focusing on an Amazon book dataset. The endeavor commences by evaluating the accuracy of the multinomial Regression model, supported by a meticulous presentation of the Confusion Matrix, enriching the analysis with profound insights into the predictive capabilities.This succinct portrayal encapsulates the pivotal stages and accomplishments within the sentiment analysis project, leveraging the expansive Amazon book dataset.]]></summary></entry><entry><title type="html">Cancer analysis - R Programming</title><link href="http://localhost:4000/project/r_data_analysis" rel="alternate" type="text/html" title="Cancer analysis - R Programming" /><published>2024-01-01T00:00:00-05:00</published><updated>2024-01-01T00:00:00-05:00</updated><id>http://localhost:4000/project/r_data_analysis</id><content type="html" xml:base="http://localhost:4000/project/r_data_analysis"><![CDATA[<p target="blank">In the Comprehensive Health Data Analysis project for the American Health Association, spearheaded an exhaustive examination of a multifaceted dataset incorporating socio-economic, demographic, and health-related variables at the county level in the United States. By meticulously mapping the data analysis process and conducting Tidy Sanity Checks, have ensured the reliability of subsequent analysis. Thorough data cleaning, inclusive of handling duplicates and outliers, set the stage for a robust exploration of key variables through hierarchical cluster analysis. This method unveiled intricate patterns, shedding light on the socio-economic and health-related determinants influencing cancer incidence and mortality rates. The analysis underscored the pivotal role of factors such as average annual cancer counts, deaths, and socio-economic landscapes in shaping health outcomes. The project’s insights have far-reaching implications for targeted cancer prevention and management strategies. My next steps involve proposing data-driven strategies for the American Health Association and delving into predictive modeling to anticipate future cancer trends. This project exemplifies my adeptness in deriving actionable insights, offering valuable contributions to public health initiatives.</p>
<h6 id="source-code--cancer_analysisr">Source Code : <a href="https://github.com/hemabhaskar/Project/blob/main/Cancer_analysis.r"><code class="language-plaintext highlighter-rouge">Cancer_analysis.r</code></a></h6>

<h2 id="key-steps-and-analyses">Key Steps and Analyses</h2>

<h3 id="data-analysis-process-mapping">Data Analysis Process Mapping</h3>
<p>Developed a comprehensive process map outlining the workflow for the entire data analysis project.</p>

<image src="https://raw.githubusercontent.com/hemabhaskar/portfolio/gh-pages/assets/images/process_map.gif" width="480" height="259" frameborder="0" class="giphy-embed" allowfullscreen=""></image>

<h3 id="tidy-sanity-checks">Tidy Sanity Checks</h3>
<p>Ensured data tidiness by performing sanity checks to validate the structure and format.</p>

<h3 id="data-cleaning">Data Cleaning</h3>
<p>Conducted thorough data cleaning, including handling duplicates, addressing missing values, and identifying and managing outliers.</p>

<h3 id="hierarchical-cluster-analysis">Hierarchical Cluster Analysis</h3>
<p>Utilized hierarchical cluster analysis to uncover patterns and relationships within the dataset, facilitating a deeper understanding of the socio-economic and health-related factors influencing cancer incidence and mortality rates.</p>

<p>In the hierarchical clustering analysis focusing on the average death rate by state, I employed a meticulous approach to distill meaningful insights. Beginning with the aggregation of death rate means at the state level, I sought to encapsulate a comprehensive overview of mortality trends. The subsequent computation of a distance matrix facilitated the understanding of the dissimilarities between states in terms of death rates. Leveraging the average linkage method, I applied hierarchical clustering, creating a dendrogram that visually depicted the hierarchical relationships among states.</p>

<p>To enhance interpretability, the dendrogram was segmented into six distinct clusters, each representing a cohesive group of states sharing similar death rate patterns. Adding a rectangular box to demarcate these clusters provided a clear visual guide for cluster identification. Notably, the analysis highlighted Kentucky as a state with the highest death rates, aligning it with the characteristics of cluster 6. This detailed hierarchical clustering approach not only unraveled the inherent structure within the death rate data but also offered a nuanced perspective on state-wise variations, showcasing the potential for targeted interventions and healthcare resource allocation.</p>

<image src="https://raw.githubusercontent.com/hemabhaskar/portfolio/gh-pages/assets/images/Dendrogram.png" width="480" height="259" frameborder="0" allowfullscreen=""></image>

<image src="https://raw.githubusercontent.com/hemabhaskar/portfolio/gh-pages/assets/images/us_map.png" width="480" height="259" frameborder="0" allowfullscreen=""></image>

<h3 id="key-variables-analyzed">Key Variables Analyzed</h3>

<ul>
  <li>
    <p>Health Indicators
<br />Explored variables such as avgAnnCount, avgDeathsPerYear, deathRate, incidenceRate, and studyPerCap to understand cancer-related statistics at the county level.</p>
  </li>
  <li>
    <p>Socio-Economic Factors
<br />Investigated variables like AvgHouseholdSize, binnedInc, BirthRate, MedianAge, medianIncome, PctBachDeg18_24, PctEmployed16_Over, PctMarriedHouseholds, and more to assess the socio-economic landscape.</p>
  </li>
  <li>
    <p>Demographic Composition
<br />Explored variables such as PctAsian, PctBlack, PctWhite, PctPrivateCoverage, PctPublicCoverage, povertyPercent, and popEst2015 to understand the demographic composition and healthcare coverage.</p>
  </li>
</ul>

<h3 id="insights-and-recommendations">Insights and Recommendations</h3>

<p>Identified key socio-economic and demographic factors influencing cancer outcomes.
Uncovered patterns in health indicators that can inform targeted interventions for cancer prevention and management.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="informative" /><category term="technology" /><summary type="html"><![CDATA[The hierarchical cluster analysis aimed to reveal geographic patterns of cancer mortality across US states. By analyzing death rate data and computing distances, it identified clusters with similar mortality profiles. The resulting dendrogram and US map visualization illustrated hierarchical relationships and regional disparities, aiding policymakers in targeting interventions to reduce cancer mortality rates nationwide.]]></summary></entry><entry><title type="html">Adventure works Dashboard - PowerBI</title><link href="http://localhost:4000/project/adworks" rel="alternate" type="text/html" title="Adventure works Dashboard - PowerBI" /><published>2023-12-01T00:00:00-05:00</published><updated>2023-12-01T00:00:00-05:00</updated><id>http://localhost:4000/project/adworks</id><content type="html" xml:base="http://localhost:4000/project/adworks"><![CDATA[<h2 id="overview">Overview</h2>
<p>Conducted an in-depth analysis of Adventure Works sales data to derive meaningful insights and identify trends. The project involved hypothesis testing, profit analysis, sales analysis, and order analysis to provide a comprehensive understanding of the company’s performance.</p>

<h6 target="blank" id="source-code--adventureworks_salespbix">Source Code : <a href="https://github.com/hemabhaskar/Project/blob/main/Adventureworks_sales.pbix"><code class="language-plaintext highlighter-rouge">Adventureworks_sales.pbix</code></a></h6>

<h2 id="hypothesis-testing">Hypothesis Testing</h2>
<h3 id="hypothesis---bikes-category-as-a-profitable-venture">Hypothesis - Bikes Category as a Profitable Venture</h3>

<p>Proposed hypothesis: The bikes category significantly contributes to Adventure Works’ profit and sales growth.
Year-over-year analysis and monthly revenue growth comparison were utilized.</p>

<h3 id="findings">Findings</h3>
<ul>
  <li>Profit Analysis
Visualized profit analysis by month and financial year for the bike category.
Identified a dip in profit across all financial years in June, with May 2020 being the most profitable ($781K) and June 2020 recording the lowest profit ($72K).
Subcategory drill-down highlighted Touring bikes as the most profitable in 2020.
Variance analysis revealed higher variance in 2018 and no profit in 2020, resulting in a negative variance.</li>
</ul>

<image src="https://raw.githubusercontent.com/hemabhaskar/portfolio/gh-pages/assets/images/Adventuresales_profit.png" width="480" height="259" frameborder="0" allowfullscreen=""></image>

<ul>
  <li>Sales Analysis
Confirmed consistent year-over-year growth in bike sales.
Highlighted a drop in bike sales in February and November 2019 compared to the previous financial year.
Annual sales for bikes exhibited a continuous upward trend, and the subcategory analysis indicated a 99% increase in new bike sales.</li>
</ul>

<image src="https://raw.githubusercontent.com/hemabhaskar/portfolio/gh-pages/assets/images/Adventuresales_Order.png" width="480" height="259" frameborder="0" allowfullscreen=""></image>

<ul>
  <li>Order Analysis
Revealed high order volumes in February, July, and November.
Introduced a new subcategory, Touring bikes, in FY2020, contributing positively to order volume and count.</li>
</ul>
<image src="https://raw.githubusercontent.com/hemabhaskar/portfolio/gh-pages/assets/images/Adventuresales_Order.png" width="480" height="259" frameborder="0" allowfullscreen=""></image>

<h3 id="key-takeaways-and-inference">Key Takeaways and Inference</h3>
<p>The positive hypothesis that the bikes category significantly contributes to profit and sales growth is supported by the analysis.
May 2020 recorded the highest profit, with Touring bikes emerging as a key contributor.
Bike sales consistently increased year-over-year, with the introduction of new bikes further boosting performance.
Strategic insights derived from order analysis, emphasizing peak order months and the impact of introducing new subcategories.</p>
<h3 id="next-steps">Next Steps:</h3>
<p>Highlight the strategic implications of these findings, propose actionable recommendations, and demonstrate your ability to turn data insights into business decisions.</p>]]></content><author><name></name></author><category term="how to" /><category term="setup" /><category term="theme" /><summary type="html"><![CDATA[Conducted an in-depth analysis of Adventure Works sales data to derive meaningful insights and identify trends. The project involved hypothesis testing, profit analysis, sales analysis, and order analysis to provide a comprehensive understanding of the company's performance.]]></summary></entry></feed>